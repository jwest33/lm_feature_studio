# -*- coding: utf-8 -*-
"""Tutorial: Gemma Scope 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NhWjg7n0nhfW--CjtsOdw5A5J_-Bzn4r

# Gemma Scope 2 Tutorial

This is a tutorial on how to use [Gemma Scope 2](https://huggingface.co/google/gemma-scope-2), Google DeepMind's suite of Sparse Autoencoders (SAEs) and Transcoders (TCs) on every layer of every model in the Gemma V3 family, as well as several multi-layer models.

Sparse Autoencoders are interpretability tools that act like a "microscope" on language model activations. They let us zoom in on dense, compressed activations, and expand them to a larger but sparser and seemingly more interpretable form, which can be a very useful tool when doing interpretability research! Transcoders and multi-layer models expand on this by helping us find not just individual concepts but circuits connecting concepts together, unlocking understanding of more complex behaviours.

**Learn more:**
* If you want to learn about Gemma Scope without writing any code, check out [this interactive demo](https://neuronpedia.org/gemma-scope-2) courtesy of [Neuronpedia](https://neuronpedia.org).
* For an overview of Gemma Scope check out [the blog post](https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/).
* See [the technical report](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/Gemma_Scope_2_Technical_Paper.pdf) for the technical details

For illustrative purposes, we begin with a lightweight tutorial that uses as few libraries as possible to outline how Gemma Scope works, and what Sparse Autoencoders are doing. This is deliberately a fairly minimalist tutorial, designed to make clear what is actually going on, but does not model research best practices.

For any serious research with Gemma Scope, **we recommend using the [SAELens](https://jbloomaus.github.io/SAELens/) and [TransformerLens](https://transformerlensorg.github.io/TransformerLens/) libraries**, see [this tutorial](https://colab.research.google.com/github/jbloomAus/SAELens/blob/main/tutorials/tutorial_2_0.ipynb) on how to use [SAELens](https://jbloomaus.github.io/SAELens/) in practice.

## Loading the Model

First, let's load the model:

For simplicity we do this straight from [HuggingFace transformers](https://huggingface.co/docs/transformers/en/index), rather than using an interpretability focused library like [TransformerLens](https://transformerlensorg.github.io/TransformerLens/) or [nnsight](https://nnsight.net/).
"""

from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer
from huggingface_hub import hf_hub_download, notebook_login
import numpy as np
import einops
import textwrap
from typing import Literal
import plotly.express as px
from functools import partial
import dataclasses
from IPython.display import display, HTML
import gc
import pandas as pd
from safetensors.torch import load_file
import torch
import torch.nn as nn

"""We load Gemma 3 1B, the second smallest model that Gemma Scope 2 works for (you can also try Gemma 3 270m, but in a Colab the 1B-size model should work fine).

We load the base model, not the chat model, since that's where our SAEs are trained. Though the SAEs seem to transfer OK to these models. First, you'll need to authenticate with huggingface in order to download the model weights (you only need a **READ** token; no other permissions).
"""

notebook_login()

torch.set_grad_enabled(False) # avoid blowing up mem

model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-3-1b-pt",
    device_map='auto',
)
tokenizer =  AutoTokenizer.from_pretrained("google/gemma-3-1b-pt")

"""Now we've loaded the model, let's try running it! We give it a prompt (which we'll come back to later) and print the generated output:"""

# The input text
prompt_physics = "The law of conservation of energy states that energy cannot be created or destroyed, only transformed."

# Use the tokenizer to convert it to tokens
# Note that this implicitly adds a special "Beginning of Sequence" or <bos> token to the start
inputs_physics = tokenizer.encode(prompt_physics, return_tensors="pt", add_special_tokens=True).to("cuda")
print(inputs_physics)

# Pass it in to the model and generate text
outputs = model.generate(input_ids=inputs_physics, max_new_tokens=50)
output_str = tokenizer.decode(outputs[0])
print()
print(textwrap.fill(output_str))

"""This was the pretrained (PT) model, so it doesn't respond like a chatbot - it just continues based on its priors for what is likely to follow the initial prompt, given the dataset it was trained on.

We'll also be using the instruction-tuned (IT) model, which behaves more like a standard chatbot.Let's also load that in and see how it works. Note that we have to carefully format the input so that it's in the correct form for our IT model:
"""

model_it = AutoModelForCausalLM.from_pretrained(
    "google/gemma-3-1b-it",
    device_map='auto',
)

def format_prompt(user_prompt: str) -> str:
  return f"""<start_of_turn>user
{user_prompt}<end_of_turn>
<start_of_turn>model
"""

user_prompt = "What is your name?"
it_inputs = tokenizer.encode(format_prompt(user_prompt), return_tensors="pt", add_special_tokens=True).to("cuda")

outputs = model_it.generate(input_ids=it_inputs, max_new_tokens=40)
print(tokenizer.decode(outputs[0]))

"""## Sparse Autoencoders

OK, so we have got our Gemma model loaded, and we can sample from it to get sensible stuff. Now, let's load one of our SAEs.

GemmaScope actually contains over four hundred SAEs, but for now we'll just load one on the residual stream at the end of layer 20 (of 26, note that layers start at 0 so this is the 21st layer. This is a fairly late layer, so the model should have time to find more abstract concepts!).

See [the final section](https://colab.research.google.com/drive/17dQFYUYnuKnP6OwQPH9v_GSYUW5aj-Rp?authuser=2#scrollTo=E7zjkVseLSPp) for more information on how to load all the other SAEs in Gemma Scope

<details><summary>What is the residual stream?</summary>

Transformers have skip connections, which means that the output of each block is the output of each sublayer *plus* the input to the block. This means that each sublayer (attention or MLP) actually only has a fairly small effect on the output of the block, since most of it comes from all the earlier layers. We call the output of a block (including skip connections) the **residual stream**.

Everything communicated from earlier layers to later layers must go via the residual stream, so it acts as a "bottleneck" in the transformer, essentially capturing everything the model has "thought" so far. This means it is often a natural thing to study, since it will contain everything important going on in the model.
</details>
"""

LAYER = 22  # options are {7, 13, 17, 22}
WIDTH = "16k"   # options are {16k, 65k, 262k, 1m}
L0 = "medium"  # options are {small, medium, big}

path_to_params = hf_hub_download(
    repo_id="google/gemma-scope-2-1b-pt",
    filename=f"resid_post/layer_{LAYER}_width_{WIDTH}_l0_{L0}/params.safetensors",
)

params = load_file(path_to_params)

"""Our SAEs are **JumpReLU** SAEs, meaning they are a standard 2-layer neural network with a JumpReLU activation function (a ReLU with a discontinuous jump).

<!-- The mapping from input to hidden activations is defined by the weight and bias parameters `W_enc` and `b_enc`, and the mapping from hidden activations back to reconstructed input is defined by `W_dec` and `b_dec`. The `threshold` parameter determines the size of the discontinuity for JumpReLU. -->

### Implementing the SAE

We now define the forward pass of the SAE for pedagogical purposes (in practice, we recommend using the implementation in SAELens).

We have 5 important parameters below:

- `w_enc`, the encoder matrix (which maps from inputs to pre-activation latent values)
- `b_enc`, the bias added onto these pre-activation latent values
- `threshold`, which determines how we apply our JumpReLU activation function
- `w_dec`, the decoder matrix (which maps from post-ReLU latent values to reconstructed activations)
- `b_dec`, the bias which is added to the final reconstruction

You can ignore `affine_skip_connection` for now; we'll come back to it in the "transcoders" section.
"""

class JumpReLUSAE(nn.Module):
  def __init__(self, d_in, d_sae, affine_skip_connection=False):
    # Note that we initialise these to zeros because we're loading in pre-trained weights.
    # If you want to train your own SAEs then we recommend using blah
    super().__init__()
    self.w_enc = nn.Parameter(torch.zeros(d_in, d_sae))
    self.w_dec = nn.Parameter(torch.zeros(d_sae, d_in))
    self.threshold = nn.Parameter(torch.zeros(d_sae))
    self.b_enc = nn.Parameter(torch.zeros(d_sae))
    self.b_dec = nn.Parameter(torch.zeros(d_in))
    if affine_skip_connection:
      self.affine_skip_connection = nn.Parameter(torch.zeros(d_in, d_in))
    else:
      self.affine_skip_connection = None

  def encode(self, input_acts):
    pre_acts = input_acts @ self.w_enc + self.b_enc
    mask = (pre_acts > self.threshold)
    acts = mask * torch.nn.functional.relu(pre_acts)
    return acts

  def decode(self, acts):
    return acts @ self.w_dec + self.b_dec

  def forward(self, x):
    acts = self.encode(x)
    recon = self.decode(acts)
    if self.affine_skip_connection is not None:
      return recon + x @ self.affine_skip_connection
    return recon

d_model, d_sae = params["w_enc"].shape
sae = JumpReLUSAE(d_model, d_sae)
sae.load_state_dict(params)
sae.cuda()

"""### Running the SAE on model activations

Let's first get out some activations from the model at the SAE target site. We'll demonstrate how to do this 'manually' first, by using Pytorch hooks. Note that this is not particularly good practice, and it's probably more practical to use a library like TransformerLens to handle hooking the SAE into a model forward pass. But for illustrative purposes, it's useful to see how it's done.

We can gather activations at a site by registering a hook. To keep this local, we can wrap this in a function that registers a hook, runs the model, saving the intermediate activation, then removes the hook. (This is basically what TransformerLens is doing under the hood)
"""

def gather_acts_hook(mod, inputs, outputs, cache: dict, key: str, use_input: bool):
  """Generic hook function whic stores activations (either input or output of a particular PyTorch module)."""
  acts = inputs[0].squeeze(0) if use_input else outputs[0]  # inputs usually have a batch dim
  cache[key] = acts
  return outputs


def gather_residual_activations(model, target_layer, inputs):

  cache = {}

  # Add a hook function to store the output of this layer of the model
  handle = model.model.layers[target_layer].register_forward_hook(
      partial(gather_acts_hook, cache=cache, key="resid_post", use_input=False)
  )

  # Forward pass inside a try/except/finally block (useful just in case our hook breaks
  # and we can't remove it!)
  try:
    _ = model.forward(inputs)
  finally:
    handle.remove()

  return cache["resid_post"]

target_act = gather_residual_activations(model, LAYER, inputs_physics)

"""Now, we can run our SAE on the saved activations."""

sae_acts = sae.encode(target_act.to(torch.float32))
recon = sae.decode(sae_acts)

"""Let's just double check that the model looks sensible by checking that we explain a decent chunk of the variance:"""

reconstruction_mse = torch.mean((recon[:, 1:] - target_act[:, 1:].float()) ** 2)
target_variance = target_act[:, 1:].float().var()

fvu = reconstruction_mse / target_variance
print(f"Fraction of variance unexplained: {fvu:.2%}")

"""This looks pretty good!

This SAE is supposed to have an L0 of ~60 (size "medium"), so let's check that too:
"""

l0_per_token = (sae_acts > 1).sum(-1)[0]
print(l0_per_token.tolist())

print(f"Average L0: {l0_per_token[1:].float().mean():.2f}")

"""It's always worth checking this sort of thing when you do this by hand to check that you haven't got the wrong site, or are missing a scaling factor or something like this. But here, our results all look like they are supposed to .

Note that there's a bit of a gotcha here; our SAEs are *NOT* trained on the BOS token, because we found that this tended to be a large outlier and to mess up training. So they tend to give nonsense when we apply to them to it, and we need to be careful not to do this accidentally! We can see this above : the BOS token is a total outlier in terms of L0!

Another way we can evaluate our SAE is by looking at the **delta loss**, i.e. how much the model's prediction loss increases when we patch in the SAE's output. To do this we'll set up a new hook function:
"""

def fwd_pass_with_sae_intervention(model, sae, target_layer, inputs):
  # Forward pass to get logits & hidden activations
  model_output_clean = model.forward(inputs, output_hidden_states=True)
  logits_clean = model_output_clean.logits[0]  # (seq, d_vocab)
  input_acts = model_output_clean.hidden_states[target_layer + 1][0]  # (seq, d_model)

  # Get the SAE reconstruction
  recon = sae.forward(input_acts.to(torch.float32))

  def intervene_on_target_act_hook(mod, inputs, outputs):
    outputs[0][0, 1:] = recon[1:]
    return outputs

  handle = model.model.layers[target_layer].register_forward_hook(intervene_on_target_act_hook)
  try:
    model_output = model.forward(inputs)
  finally:
    handle.remove()

  # Get logits from this corrupted forward pass
  logits = model_output.logits[0]

  return logits_clean, logits


def cross_entropy_loss(logits: torch.Tensor, tokens: torch.Tensor) -> torch.Tensor:
  """Measures avg cross entropy loss."""
  logprobs = logits[:-1].log_softmax(dim=-1)
  tokens = tokens[1:]
  correct_logprobs = logprobs[torch.arange(len(tokens)), tokens]
  return -correct_logprobs

logits_clean, logits_sae = fwd_pass_with_sae_intervention(model, sae, LAYER, inputs_physics)
loss_clean = cross_entropy_loss(logits_clean, inputs_physics[0])
loss_sae = cross_entropy_loss(logits_sae, inputs_physics[0])

print(f"Loss (clean): {loss_clean.mean():.4f}")
print(f"Loss (corrupted): {loss_sae.mean():.4f}")
print(f"Delta loss: {loss_sae.mean() - loss_clean.mean():.4f}")

"""Let's look at the highest activating features on this input text, on each token position:"""

top_activations, top_features = sae_acts.max(-1)

top_features

"""Note that a lot of these indices are quite small, relative to the number of features in our SAE (over 200 thousand). This is because our SAE was trained with [**Matryoshka loss**](https://www.lesswrong.com/posts/zbebxYCqsryPALh8C/matryoshka-sparse-autoencoders), which imposes a feature hierarchy: the smaller-indexed features are incentivised to be good at reconstructing the input even when all other features are switched off. This helps avoid problems like **feature absorption**.

<!-- Now, let's take a particular word in our prompt from earlier, and see which feature causes it to fire strongest: -->

<!-- It seems like **1473** is a pretty common top-activating feature in the sequence above, so let's inspect this feature. First, we'll see exactly which tokens in our prompt it fires on: -->

<!-- Let's inspect this feature. First, we'll see exactly which tokens in our prompt it fires on: -->

Let's find the feature which activates the strongest when averaged over all tokens in the sequence:

<!--
gravity_idx = tokenizer.tokenize(prompt_newton, add_special_tokens=True).index("▁universe")
print(f"Token idx: {gravity_idx}")

feature_idx = top_features.squeeze().tolist()[gravity_idx]
print(f"Top activating feature: {feature_idx}")

feature_idx = top_features.squeeze().mean(0).argmax()
print(f"Top activating feature: {feature_idx}")
-->
"""

top_acts, top_latents = sae_acts.squeeze().mean(0).topk(5)

for act, idx in zip(top_acts, top_latents):
  print(f"{act:>6.1f} | {idx}")

"""Latent 10679 seems to fire strongest. Let's inspect it:"""

feature_idx = 10679

str_toks = tokenizer.tokenize(prompt_physics, add_special_tokens=True)
activations = sae_acts[0, :, feature_idx].tolist()

def html_activations(str_toks: list[str], activations: list[float]):
  return "".join(
      f'<span style="background-color: rgba(255,0,0,{v}); padding: 4px 0px;">{t}</span>'
      for t, v in zip(str_toks, np.array(activations) / (1e-6 + np.max(activations)), strict=True)
  )

display(HTML(html_activations(str_toks, activations)))

"""One guess we might have is that this latent fires on concepts related to science or scientific laws. Let's test this out with a few examples:"""

for prompt in [
    "Gemma Scope 2 is a model release from Google DeepMind",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit",
    "Gravity describes how massive objects attract one another",
    "A charge accelerating through an electric field experiences a force",
    "Chemical fuel stores energy in molecular bonds, which is released"
]:
  inputs = tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=True).to("cuda")
  tgt_acts = gather_residual_activations(model, LAYER, inputs)

  sae_acts = sae.encode(tgt_acts.to(torch.float32))

  str_toks = tokenizer.tokenize(prompt, add_special_tokens=True)
  display(HTML(html_activations(str_toks, sae_acts[0, :, feature_idx].tolist())))
  print()

"""Okay, so it doesn't fire on the gravity sentence, but it does fire on both the other physics-related sentences as soon as they start talking about forces, energies or fields. This gives us a more specific idea of the concepts this latent might represent.

<!-- This theory seems reasonable: it fires on both the sentences related to physical forces (note that it doesn't seem to just be a "gravity" latent given how it fires on the second of these two sentences). -->

<!-- Okay, so it seems like this might be the case, although the activation is more consistent when describing gravity than on other forces. -->

We can investigate this further by looking at the latent's unembedding, in other words **what words get predicted strongest when this latent fires.**
"""

w_u = model.lm_head.weight  # shape (d_vocab, d_model)

decoder_vector = sae.w_dec[feature_idx]  # shape (d_model,)

top_activations, top_tokens = torch.topk(w_u @ decoder_vector, k=10)

for act, tok in zip(top_activations, top_tokens):
  print(f"{act:.4f} | {tokenizer.decode(tok)}")

"""<!-- This definitely increases credence in our theory that this feature specifically relates to gravity! -->

Lastly, we can try **steering with this feature**. This means intervening in the residual stream of the model to add some multiple of this feature's decoder vector, so that we can change the behaviour of the model during generation.

You should see that when we steer the model on this "physical force feature", it starts talking more about physics (specifically forces like electromagnetism or gravity). Note that steering can often be fragile; it's difficult to choose the intervention layer and steering coefficient in a way that gives the expected behavioural change without also breaking the model's coherence. If you're curious, you can try increasing the `coeff` parameter below and seeing what happens!
"""

def generate_with_steering(model, sae, inputs, target_layer, feature_idx: int, coeff: float):

  def steering_hook(mod, inputs, outputs):
    output = outputs[0]
    # We have to be careful about KV caching! This logic handles different cases depending on
    # whether this is the first forward pass or a cached pass.
    if output.shape[1] == 1:
      avg_norm = torch.norm(output, dim=-1)
      output += coeff * avg_norm * sae.w_dec[feature_idx]
    else:
      avg_norm = torch.norm(output[0, 1:], dim=-1, keepdim=True)
      output[0, 1:] += coeff * avg_norm * sae.w_dec[feature_idx]

    return outputs

  handle = model.model.layers[target_layer].register_forward_hook(steering_hook)
  try:
    outputs = model.generate(input_ids=inputs, max_new_tokens=80, do_sample=False)
    output_str = tokenizer.decode(outputs[0])
  finally:
    handle.remove()

  return output_str.split("<start_of_turn>model")[1].strip()


user_prompt = "Tell me a fun fact."
inputs = tokenizer.encode(format_prompt(user_prompt), return_tensors="pt", add_special_tokens=True).to("cuda")

print(user_prompt)
print("======================= NO STEERING =======================")
output_str = generate_with_steering(
    model=model_it,
    sae=sae,
    inputs=inputs,
    target_layer=LAYER - 5,
    feature_idx=feature_idx,
    coeff=0.0,
)
print(textwrap.fill(output_str))
print("======================= STEERING =======================")
output_str_steered = generate_with_steering(
    model=model_it,
    sae=sae,
    inputs=inputs,
    target_layer=LAYER - 5,
    feature_idx=feature_idx,
    coeff=0.25,
)
print(textwrap.fill(output_str_steered))

"""Note that steering is expected to be pretty brittle with smaller models. Generally, larger models (up to a certain point) can better express more complex concepts and are easier to steer without breaking coherence.

As an exercise, try finding more latents to steer with. Can you come up with any other interesting prompts and latents?

## Transcoders

We've also trained a suite of **transcoders** for this release. A transcoder is very similar to an SAE, except rather than reconstructing an activation vector, it reconstructs the mapping from input vector to some output (commonly the input and output of an MLP layer). In this way, rather than decomposing a model's **representations**, it decomposes a model's **computations**.

Note - this is where we introduce a new weight, the **affine skip connection**. This is a learned linear transformation from the input to output activations of the transcoder. You can view it as the learned linear component of the MLP layer, and the latents represent the nonlinear components.
"""

LAYER = 17
WIDTH = "16k"
L0 = "medium"

filename=f"transcoder/layer_{LAYER}_width_{WIDTH}_l0_{L0}_affine/params.safetensors"
print(filename)

path_to_params = hf_hub_download(
    repo_id="google/gemma-scope-2-1b-pt",
    filename=f"transcoder/layer_{LAYER}_width_{WIDTH}_l0_{L0}_affine/params.safetensors",
)

params = load_file(path_to_params)

d_model, d_sae = params["w_enc"].shape
transcoder = JumpReLUSAE(d_model, d_sae, affine_skip_connection=True)
transcoder.load_state_dict(params)
transcoder.cuda()

"""Once nice property about transcoders is that you can use them to find **circuits**. This is because (if you freeze attention patterns) we can model the relationship between two transcoder latents in different layers as being totally **linear**."""

def gather_transcoder_activations(model, target_layer, inputs):

  cache = {}

  handle_input = model.model.layers[target_layer].pre_feedforward_layernorm.register_forward_hook(
      partial(gather_acts_hook, cache=cache, key="transcoder_input", use_input=False)
  )
  handle_target = model.model.layers[target_layer].post_feedforward_layernorm.register_forward_hook(
      partial(gather_acts_hook, cache=cache, key="transcoder_target", use_input=False)
  )

  try:
    _ = model.forward(inputs)
  finally:
    handle_input.remove()
    handle_target.remove()

  return cache

prompt = "The quick brown fox jumped over the lazy dog"
inputs = tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=True).to("cuda")

cache = gather_transcoder_activations(model, LAYER, inputs)

sae_input = cache["transcoder_input"].to(torch.float32).cuda()
sae_target = cache["transcoder_target"].to(torch.float32).cuda()

sae_acts = transcoder.encode(sae_input)
recon = transcoder.forward(sae_input)

mse = torch.mean((recon[1:] - sae_target[1:].float())**2)
var = sae_target[1:].float().var()
fvu = mse / var
l0 = (sae_acts[1:] > 0).float().sum(-1).mean()

print(f"L0: {l0:.2f}")
print(f"Fraction of variance unexplained: {mse / var:.2%}")

"""This is a higher FVU than our sparse autoencoder. But since the output of a single MLP is a less important causal node than the residual stream (which contains **all accumulated information** up to that layer in the model), transcoders usually have a smaller delta loss when we patch in their output.

Let's test this, with a slight modification of our previous patching function:
"""

def fwd_pass_with_sae_intervention(model, sae, target_layer, inputs):

  # Forward pass to get clean logits, plus activations for intervention:

  input_acts = None

  def cache_inputs(mod, inputs, outputs):
    """This will store the transcoder input (i.e. the pre-MLP layernorm output)."""
    nonlocal input_acts
    input_acts = outputs[0]
    return outputs

  handle_caching = model.model.layers[target_layer].pre_feedforward_layernorm.register_forward_hook(
      cache_inputs
  )
  try:
    model_output_clean = model.forward(inputs)
  finally:
    handle_caching.remove()

  # Forward pass to get corrupted logits, from injecting SAE output at target site:

  recon = sae.forward(input_acts[1:].to(torch.float32))

  def inject_outputs(mod, inputs, outputs):
    """This will patch the SAE's reconstruction into the model's MLP output."""
    output = outputs[0]
    output[1:] = recon
    return outputs

  handle_injecting = model.model.layers[target_layer].post_feedforward_layernorm.register_forward_hook(
      inject_outputs
  )
  try:
    model_output_corrupted = model.forward(inputs)
  finally:
    handle_injecting.remove()

  return model_output_clean.logits[0], model_output_corrupted.logits[0]


def cross_entropy_loss(logits: torch.Tensor, tokens: torch.Tensor) -> torch.Tensor:
  """Measures avg cross entropy loss."""
  logprobs = logits[:-1].log_softmax(dim=-1)
  tokens = tokens[1:]
  correct_logprobs = logprobs[torch.arange(len(tokens)), tokens]
  return -correct_logprobs

logits_clean, logits_sae = fwd_pass_with_sae_intervention(model, transcoder, LAYER, inputs_physics)
loss_clean = cross_entropy_loss(logits_clean, inputs_physics[0])
loss_sae = cross_entropy_loss(logits_sae, inputs_physics[0])

print(f"Loss (clean): {loss_clean.mean():.4f}")
print(f"Loss (corrupted): {loss_sae.mean():.4f}")
print(f"Delta loss: {loss_sae.mean() - loss_clean.mean():.4f}")

str_toks = tokenizer.tokenize(tokenizer.decode(inputs_physics[0]))

df = {"token": list(range(len(str_toks)-1)), "Clean": loss_clean.tolist(), "SAE": loss_sae.tolist()}
px.line(
    df, x="token", y=["Clean","SAE"], labels={"value": "Loss"}
).update_layout(
    xaxis=dict(tickvals=df["token"], ticktext=str_toks[1:], tickangle=45),
    title="Cross-entropy loss with SAE intervention",
    width=800, height=400
).show()

"""# Other models

We finish with a speedrun of other models in the GemmaScope 2 release. These include:

- Single-layer sparse autoencoders trained on **attention output** and **MLP output**
- [Weakly-causal Crosscoders](https://transformer-circuits.pub/2024/crosscoders/index.html#cross-layer-features) trained on 4 concatenated layers of the residual stream
- [Cross-layer transcoders](https://transformer-circuits.pub/2025/attribution-graphs/methods.html), trained to reconstruct all concatenated MLP outputs from all residual stream activations

## MLP-output and attention-output SAEs
"""

def gather_mlp_out_activations(model, target_layer, inputs):
  act_cache = {}
  handle = model.model.layers[target_layer].post_feedforward_layernorm.register_forward_hook(
      partial(gather_acts_hook, key="acts", cache=act_cache, use_input=False)
  )
  try:
    _ = model.forward(inputs)
  finally:
    handle.remove()
  return act_cache.pop("acts")


def gather_attn_out_activations(model, target_layer, inputs):
  act_cache = {}
  handle = model.model.layers[target_layer].self_attn.o_proj.register_forward_hook(
      partial(gather_acts_hook, key="acts", cache=act_cache, use_input=True)
  )
  try:
    _ = model.forward(inputs)
  finally:
    handle.remove()
  return act_cache.pop("acts")

"""First, the MLP-output SAE:"""

def load_sae(category: str, layer: int, width: int, l0: str, affine: bool = False) -> JumpReLUSAE:
  affine_str = "_affine" if affine else ""
  path_to_params = hf_hub_download(
      repo_id="google/gemma-scope-2-1b-pt",
      filename=f"{category}/layer_{layer}_width_{width}_l0_{l0}{affine_str}/params.safetensors",
  )
  params = load_file(path_to_params)
  d_model, d_sae = params["w_enc"].shape
  sae = JumpReLUSAE(d_model, d_sae)
  sae.load_state_dict(params)
  return sae.cuda()

mlp_sae = load_sae(
    category="mlp_out",
    layer=17,
    width="16k",
    l0="medium"
)

prompt = "The quick brown fox jumped over the lazy dog"
inputs = tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=True).to("cuda")

sae_input = gather_mlp_out_activations(model, LAYER, inputs)

sae_acts = mlp_sae.encode(sae_input.to(torch.float32))
recon = mlp_sae.decode(sae_acts)

mse = torch.mean((recon[1:] - sae_input[1:].float())**2)
var = sae_input[1:].float().var()
fvu = mse / var
l0 = (sae_acts[1:] > 0).float().sum(-1).mean()

print(f"L0: {l0:.2f}")
print(f"Fraction of variance unexplained: {mse / var:.2%}")

"""Next, we'll look at the **attention output SAE**, which reconstructs the pre-layernorm values of the attention layer. Note that this means if you want to do any kind of circuit analysis, you should probably be freezing layernorm (and remember that these values are summed over the head dimension)."""

attn_sae = load_sae(
    category="attn_out",
    layer=17,
    width="16k",
    l0="medium"
)

prompt = "The quick brown fox jumped over the lazy dog"
inputs = tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=True).to("cuda")
sae_input = gather_attn_out_activations(model, LAYER, inputs)

sae_acts = attn_sae.encode(sae_input.to(torch.float32))
recon = attn_sae.decode(sae_acts)

mse = torch.mean((recon[1:] - sae_input[1:].float())**2)
var = sae_input[1:].float().var()
fvu = mse / var
l0 = (sae_acts[1:] > 0).float().sum(-1).mean()

print(f"L0: {l0:.2f}")
print(f"Fraction of variance unexplained: {mse / var:.2%}")

"""## Crosscoders

First, we need a new architecture, to handle stacked encoder matrices (each for a different layer) and all-to-all decoder matrices (which can map from earlier to later layers).
"""

class JumpReLUMultiLayerSAE(nn.Module):
  def __init__(self, d_in, d_sae, num_layers, affine_skip_connection=False):
    # Note that we initialise these to zeros because we're loading in pre-trained weights.
    # If you want to train your own SAEs then we recommend using blah
    super().__init__()
    self.w_enc = nn.Parameter(torch.zeros(num_layers, d_in, d_sae))
    self.w_dec = nn.Parameter(torch.zeros(num_layers, d_sae, num_layers, d_in))
    self.threshold = nn.Parameter(torch.zeros(num_layers, d_sae))
    self.b_enc = nn.Parameter(torch.zeros(num_layers, d_sae))
    self.b_dec = nn.Parameter(torch.zeros(num_layers, d_in))
    if affine_skip_connection:
      self.affine_skip_connection = nn.Parameter(torch.zeros(num_layers, d_in, d_in))
    else:
      self.affine_skip_connection = None

  def encode(self, input_acts):
    pre_acts = einops.einsum(
        input_acts, self.w_enc, "... layer d_in, layer d_in d_sae -> ... layer d_sae"
    ) + self.b_enc
    mask = (pre_acts > self.threshold)
    acts = mask * torch.nn.functional.relu(pre_acts)
    return acts

  def decode(self, acts):
    return einops.einsum(
        acts, self.w_dec, "... layer_in d_sae, layer_in d_sae layer_out d_dec -> ... layer_out d_dec"
    ) + self.b_dec

  def forward(self, x):
    acts = self.encode(x)
    recon = self.decode(acts)
    if self.affine_skip_connection is not None:
      return recon + einops.einsum(
          x, self.affine_skip_connection, "... layer d_in, layer d_in d_dec -> ... layer d_dec"
      )
    return recon

"""Next, some new logic for loading in our params (which are split over layers):"""

NUM_LAYERS = 26

def load_multi_layer_sae(
    category: Literal["clt", "crosscoder"],
    num_layers: int,
    width: str,
    l0: str,
    affine: bool = False,
    device = "cuda",
    half_precision: bool = False,
) -> JumpReLUMultiLayerSAE:

  affine_str = "_affine" if affine else ""

  params_list = []

  if category == "crosscoder":
    # Crosscoder names are e.g. "layer_7_13_17_22_width_262k_l0_medium"
    layers = [int(x * num_layers + 0.5) for x in [0.25, 0.5, 0.65, 0.85]]
    print(f"Loading crosscoder for layers {layers}")
    subcategory = f"layer_{'_'.join(str(x) for x in layers)}_width_{width}_l0_{l0}{affine_str}"
  else:
    assert category == "clt"
    # CLT names are just e.g. "width_262k_l0_medium_affine"
    print("Loading CLT for all layers")
    layers = list(range(num_layers))
    affine_str = "_affine" if affine else ""
    subcategory = f"width_{width}_l0_{l0}{affine_str}"

  for layer_idx in range(len(layers)):
    path_to_params = hf_hub_download(
        repo_id="google/gemma-scope-2-1b-pt",
        filename=f"{category}/{subcategory}/params_layer_{layer_idx}.safetensors",
    )
    params = load_file(path_to_params, device=device)
    params_list.append(params)

  # We stack all params along the leading "layer" dimension
  params = {
      k: torch.stack([params[k] for params in params_list])
      for k in params_list[0].keys()
  }
  d_model, d_sae = params["w_enc"].shape[1:]
  sae = JumpReLUMultiLayerSAE(d_model, d_sae, len(layers), affine)
  sae.load_state_dict(params)
  if half_precision:
    sae = sae.half()
  return sae

"""And now we can test out our crosscoder, which was trained on layers (7, 13, 17, 22), chosen because these are spaced 25%, 50%, 65% and 85% of the way through the model."""

crosscoder = load_multi_layer_sae(
    category="crosscoder",
    num_layers=NUM_LAYERS,
    width="262k",
    l0="medium"
)

{k: v.shape for k, v in crosscoder.named_parameters()}

crosscoder.cuda()

"""Let's look at its performance too:"""

def gather_crosscoder_activations(model, target_layers, inputs):
  act_cache = {}
  handles = []
  for layer in target_layers:
    handle = model.model.layers[layer].register_forward_hook(
        partial(gather_acts_hook, key=f"acts_{layer}", cache=act_cache, use_input=False)
    )
    handles.append(handle)
  try:
    _ = model.forward(inputs)
  finally:
    for handle in handles:
      handle.remove()
  return torch.stack([act_cache[f"acts_{layer}"][0] for layer in target_layers], axis=-2)

layers = [7, 13, 17, 22]

prompt = "The quick brown fox jumped over the lazy dog"
inputs = tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=True).to("cuda")
sae_input = gather_crosscoder_activations(model, layers, inputs).to(torch.float32)

sae_acts = crosscoder.encode(sae_input)
recon = crosscoder.forward(sae_input)

mse = torch.mean((recon[1:] - sae_input[1:].float())**2)
var = sae_input[1:].float().var()
fvu = mse / var
l0 = (sae_acts[1:] > 0).float().sum((-1, -2)).mean()  # sum over (layer, feature) dims

print(f"L0: {l0:.2f}")
print(f"Fraction of variance unexplained: {mse / var:.2%}")

"""We can also see what its delta loss is, when we intervene on a single layer:"""

# Forward pass to get logits & hidden activations
model_output_clean = model.forward(inputs_physics, output_hidden_states=True)
logits_clean = model_output_clean.logits[0]  # (seq, d_vocab)

# Extract all crosscoder input activations
input_acts = torch.stack([
    model_output_clean.hidden_states[layer + 1][0]
    for layer in layers
], axis=1)  # (seq, layers, d_model)

# Get the SAE reconstruction
recon = crosscoder.forward(input_acts.to(torch.float32))

def intervene_with_crosscoder(mod, inputs, outputs, layer):
  outputs[0][layers.index(layer), 1:] = recon[1:, layers.index(layer)]
  return outputs

# Add hooks to intervene at every layer we plan to get the loss from
handles = []
for layer in layers:
  handle = model.model.layers[layer].register_forward_hook(
      partial(intervene_with_crosscoder, layer=layer)
  )
  handles.append(handle)
try:
  model_output = model.forward(
      einops.repeat(inputs_physics, "1 seq -> 4 seq")
  )
finally:
  for handle in handles:
    handle.remove()

# Get logits from this corrupted forward pass
logits_sae = model_output.logits[0]

loss_clean = cross_entropy_loss(logits_clean, inputs_physics[0])
print(f"Loss (clean): {loss_clean.mean():.4f}")

for label, logits in zip(layers, model_output.logits):
  loss_sae = cross_entropy_loss(logits, inputs_physics[0])
  delta_loss = loss_sae - loss_clean
  print(f"Delta loss at layer {label:02}: {delta_loss.mean():.4f}")

"""For a given L0 (in this case 50), we expect crosscoders to have a higher delta loss per layer than the corresponding SAE, because their sparsity budget has to be allocated across all layers.

## CLTs

Finally we'll look at CLTs - we've done most of the work for them already, since their architecture is similar to crosscoders and their activation sites are equivalent to transcoders (except concatenated).
"""

# Code to delete previous models, freeing up space:
try:
  del crosscoder, attn_sae, mlp_sae, transcoder, sae
except NameError:
  print("Already deleted.")

gc.collect()
torch.cuda.empty_cache()

clt = load_multi_layer_sae(
    category="clt",
    num_layers=NUM_LAYERS,
    width="262k",
    l0="big",
    affine=True,
    half_precision=True,
)

{k: v.shape for k, v in clt.named_parameters()}

clt.cuda()

def gather_clt_activations(model, num_layers, inputs):
  act_cache = {}
  handles = []
  for layer in range(num_layers):
    handle_input = model.model.layers[layer].pre_feedforward_layernorm.register_forward_hook(
        partial(gather_acts_hook, cache=act_cache, key=f"input_{layer}", use_input=False)
    )
    handle_target = model.model.layers[layer].post_feedforward_layernorm.register_forward_hook(
        partial(gather_acts_hook, cache=act_cache, key=f"target_{layer}", use_input=False)
    )
    handles.extend([handle_input, handle_target])
  try:
    _ = model.forward(inputs)
  finally:
    for handle in handles:
      handle.remove()

  return (
      torch.stack([act_cache[f"input_{layer}"] for layer in range(num_layers)], axis=-2),
      torch.stack([act_cache[f"target_{layer}"] for layer in range(num_layers)], axis=-2),
  )

sae_input, sae_target = gather_clt_activations(model, NUM_LAYERS, inputs_physics)
sae_input = sae_input.half()
sae_target = sae_target.half()

sae_acts = clt.encode(sae_input)
recon = clt.forward(sae_input)

mse = torch.mean((recon[1:] - sae_target[1:].float())**2)
var = sae_target[1:].float().var()
fvu = mse / var
l0 = (sae_acts[1:] > 0).float().sum((-1, -2)).mean()  # sum over (layer, feature) dims

print(f"L0: {l0:.2f}")
print(f"Fraction of variance unexplained: {mse / var:.2%}")

"""This looks like quite a high fraction of variance unexplained, but digging deeper we find this is because the FVU is much higher for later layers than it is for early layers (and late-layer MLP outputs have much higher norm). To test whether our CLT is effectively capturing the important part of the late-layer MLP outputs, we can look at the delta loss from intervening at each layer:"""

# Forward pass to get logits & hidden activations
model_output_clean = model.forward(inputs_physics, output_hidden_states=True)
logits_clean = model_output_clean.logits[0]  # (seq, d_vocab)

# Extract all crosscoder input activations
sae_input, sae_target = gather_clt_activations(model, NUM_LAYERS, inputs_physics)

# Get the SAE reconstruction
recon = clt.forward(sae_input.half())

def intervene_with_clt(mod, inputs, outputs, layer):
  outputs[layer, 1:] = recon[1:, layer]
  return outputs

# Add hooks to intervene at every layer we plan to get the loss from
handles = []
for layer in range(NUM_LAYERS):
  handle = model.model.layers[layer].post_feedforward_layernorm.register_forward_hook(
      partial(intervene_with_clt, layer=layer)
  )
  handles.append(handle)
try:
  model_output = model.forward(
      einops.repeat(inputs_physics, f"1 seq -> {NUM_LAYERS} seq")
  )
finally:
  for handle in handles:
    handle.remove()

# Get logits from this corrupted forward pass
logits_sae = model_output.logits[0]

loss_clean = cross_entropy_loss(logits_clean, inputs_physics[0])
print(f"Loss (clean): {loss_clean.mean():.4f}")

for layer, logits in enumerate(model_output.logits):
  loss_sae = cross_entropy_loss(logits, inputs_physics[0])
  delta_loss = loss_sae - loss_clean
  print(f"Delta loss at layer {layer:02}: {delta_loss.mean():.4f}")

"""## IT SAEs & displaying top activations

Here, you can load in example activations data to see what kinds of prompts maximally cause a particular feature to fire.

When we finish adding full Neuronpedia support, we'll add to this setion!
"""

def load_example_data(
    model_size: str = "27b",
    category: str = "resid_post",
    layer: int = 0,
    width: int = "262k",
    l0: str = "medium",
    affine: bool = False,
    instruction_tuned: bool = True,
) -> dict[str, np.ndarray]:
  affine_str = "_affine" if affine else ""
  repo_id=f"google/gemma-scope-2-{model_size}-{'it' if instruction_tuned else 'pt'}"
  filename=f"{category}/layer_{layer}_width_{width}_l0_{l0}{affine_str}/examples.safetensors"
  print(repo_id)
  print(filename)
  path_to_data = hf_hub_download(
      repo_id=f"google/gemma-scope-2-{model_size}-{'it' if instruction_tuned else 'pt'}",
      filename=f"{category}/layer_{layer}_width_{width}_l0_{l0}{affine_str}/examples.safetensors",
  )
  return load_file(path_to_data)

example_data = load_example_data(layer=53)

{k: v.shape for k, v in example_data.items()}

def _to_str_tokens(tokens: list[int]) -> list[str]:
  str_tokens = tokenizer.convert_ids_to_tokens(tokens)
  for i, t in enumerate(str_tokens):
    if t.startswith("▁"):
      str_tokens[i] = " " + t[1:]
  return str_tokens


def inspect_feature(
    example_data: dict[str, np.ndarray],
    global_feature: int,
    buf: tuple[int, int] = (25, 25),
    max_examples: int = 10,
    reuse_same_sequences: bool = True,
) -> None:
  """Visualizes the top-activating sequences for a particular feature."""
  tokens = example_data["tokens"]
  activations = example_data["activations"]
  positions = example_data["positions"]
  seq_ids = example_data["seq_ids"]
  feature_frequencies = example_data["feature_frequencies"]

  span = lambda s, a: (
      f'<span style="background-color: rgba(255,0,0,{a:.3f});">{s}</span>'
  )
  escape_html = lambda x: x.replace("<", "&lt;").replace(">", "&gt;")

  def join_str_list(str_toks: list[str], acts: list[float]):
    return "".join([span(s, a) for s, a in zip(str_toks, acts, strict=True)])

  # Get activations, cropped past the point where it's not actually active (we
  # just padded the array out to the same length as the other features)
  activations = activations[global_feature]
  n_acts = (activations > 0).sum().item()
  if n_acts == 0:
    print(f"No activations for feature {global_feature}")
    return
  activations = activations[:n_acts]
  seq_ids = seq_ids[global_feature][:n_acts]
  positions = positions[global_feature][:n_acts]

  # Print out frequency in 2 different ways (cached value & dataframe counts)
  print(f"Inspecting feature {global_feature}")
  print(f"Frequency: {feature_frequencies[global_feature]:.2e}")
  if "top_tokens" in example_data:
    str_tokens = _to_str_tokens(example_data["top_tokens"][global_feature])
    print(f"Top tokens: {str_tokens}")

  # Make a list of formatted sequences for each of our top activations
  top_activations = []
  formatted_sequences = []
  position_tuples = []
  max_act = max(activations[0], 1e-12)
  while len(formatted_sequences) < max_examples:
    # Finish if we don't have any more nonzero examples we can take
    if not seq_ids.shape[0]:
      break

    # Pick the max-activation sequence not yet chosen
    idx = np.argmax(activations).item()
    seq_id = seq_ids[idx].item()
    position = positions[idx].item()
    activation = activations[idx].item()
    position_tuples.append(f"{seq_id},{position}")
    top_activations.append(activation)

    # Get the string tokens, maybe adjusting the buffer if this token is too
    # close to the start or end of the sequence
    true_buf = (
        min(buf[0], position),
        min(buf[1], tokens.shape[1] - 1 - position),
    )
    str_toks = _to_str_tokens(
        tokens[seq_id, position - true_buf[0] : position + true_buf[1] + 1]
    )
    str_toks = list(map(escape_html, str_toks))
    acts = np.zeros((true_buf[1] + true_buf[0] + 1,))

    # Get the tokens & activations in a padded region around that sequence
    seq_id_mask = seq_ids == seq_id
    pos_diff = positions - position
    position_mask = (-pos_diff < true_buf[0]) & (pos_diff < true_buf[1])
    full_mask = seq_id_mask & position_mask
    for pos, act in zip(positions[full_mask], activations[full_mask]):
      acts[pos - position + true_buf[0]] = act / max_act
    formatted_sequences.append(join_str_list(str_toks, acts))

    # Filter out all other activations with the same sequence
    filter_mask = ~(full_mask if reuse_same_sequences else seq_id_mask)
    activations = activations[filter_mask]
    seq_ids = seq_ids[filter_mask]
    positions = positions[filter_mask]

  output_df = pd.DataFrame({
      "position": position_tuples,
      "activation": top_activations,
      "tokens": formatted_sequences,
  }).reset_index(drop=True)
  display(
      output_df.style.set_properties(
          subset=["tokens"], **{"font-family": "Helvetica"}
      )
  )


inspect_feature(example_data, global_feature=50705)
